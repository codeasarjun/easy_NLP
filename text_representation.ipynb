{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ca9817",
   "metadata": {},
   "source": [
    "<h2>Bag-of-Words (BoW)</h2>\n",
    "The Bag-of-Words model represents text as a collection of words, ignoring grammar and word order. It creates a vector representation where each dimension corresponds to a unique word in the vocabulary, and the value in each dimension indicates the frequency of that word in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9d334c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words representation:\n",
      "[[0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0]\n",
      " [1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1]]\n",
      "****************************\n",
      "Vocabulary:\n",
      "['amazing' 'an' 'architecture' 'beautiful' 'booked' 'eiffel' 'experience'\n",
      " 'exploring' 'flight' 'for' 'in' 'is' 'its' 'known' 'louvre' 'museum'\n",
      " 'paris' 'the' 'to' 'tower' 'visited' 'was']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    \"I booked a flight to Paris.\",\n",
    "    \"Paris is known for its beautiful architecture.\",\n",
    "    \"I visited the Eiffel Tower in Paris.\",\n",
    "    \"Exploring the Louvre Museum was an amazing experience.\",\n",
    "]\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "bow_representation = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(\"Bag-of-Words representation:\")\n",
    "print(bow_representation.toarray())\n",
    "\n",
    "# Print the vocabulary\n",
    "print('****************************')\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e5642d",
   "metadata": {},
   "source": [
    "<h2>TF-IDF (Term Frequency-Inverse Document Frequency)</h2>\n",
    "TF-IDF measures the importance of a term in a document relative to a collection of documents. It calculates the product of term frequency (TF) and inverse document frequency (IDF). TF measures how frequently a term occurs in a document, while IDF measures how important a term is across the entire corpus by penalizing frequent terms.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277f2d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF representation:\n",
      "[[0.         0.         0.         0.         0.5417361  0.\n",
      "  0.         0.         0.5417361  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.34578314 0.\n",
      "  0.5417361  0.         0.         0.        ]\n",
      " [0.         0.         0.39505606 0.39505606 0.         0.\n",
      "  0.         0.         0.         0.39505606 0.         0.39505606\n",
      "  0.39505606 0.39505606 0.         0.         0.25215917 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.44592216\n",
      "  0.         0.         0.         0.         0.44592216 0.\n",
      "  0.         0.         0.         0.         0.28462634 0.35157015\n",
      "  0.         0.44592216 0.44592216 0.        ]\n",
      " [0.36222393 0.36222393 0.         0.         0.         0.\n",
      "  0.36222393 0.36222393 0.         0.         0.         0.\n",
      "  0.         0.         0.36222393 0.36222393 0.         0.2855815\n",
      "  0.         0.         0.         0.36222393]]\n",
      "********************\n",
      "Vocabulary:\n",
      "['amazing' 'an' 'architecture' 'beautiful' 'booked' 'eiffel' 'experience'\n",
      " 'exploring' 'flight' 'for' 'in' 'is' 'its' 'known' 'louvre' 'museum'\n",
      " 'paris' 'the' 'to' 'tower' 'visited' 'was']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    \"I booked a flight to Paris.\",\n",
    "    \"Paris is known for its beautiful architecture.\",\n",
    "    \"I visited the Eiffel Tower in Paris.\",\n",
    "    \"Exploring the Louvre Museum was an amazing experience.\",\n",
    "]\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "tfidf_representation = vectorizer.fit_transform(corpus)\n",
    "\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "print(\"TF-IDF representation:\")\n",
    "print(tfidf_representation.toarray())\n",
    "\n",
    "print(\"********************\")\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef1944",
   "metadata": {},
   "source": [
    "<h2>Word Embeddings</h2>\n",
    "Word embeddings represent words as dense vectors in a continuous vector space, where semantically similar words are mapped to nearby points. These embeddings capture the semantic relationships between words and can be used to derive meaning from the text.\n",
    "example: (Word2Vec, GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45ac6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73c9a23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings:\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = [\n",
    "    \"Cricket is a popular sport played with a bat and ball.\",\n",
    "    \"The Ashes series is one of the most famous rivalries in cricket.\",\n",
    "    \"Sachin Tendulkar is considered one of the greatest cricketers of all time.\",\n",
    "    \"The World Cup is the pinnacle of international cricket tournaments.\",\n",
    "    \"Playing a test match can last up to five days, with each team having two innings.\",\n",
    "    \"In Twenty20 cricket, each team has a maximum of 20 overs to score as many runs as possible.\",\n",
    "    \"The term 'hat-trick' refers to a bowler taking three wickets in three consecutive deliveries.\",\n",
    "    \"Australia has won the ICC Cricket World Cup multiple times.\",\n",
    "    \"The DRS system (Decision Review System) is used to review umpiring decisions in international cricket matches.\",\n",
    "    \"Fielding positions in cricket include roles such as slip, gully, and cover.\",\n",
    "]\n",
    "\n",
    "\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "\n",
    "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "\n",
    "word_embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "\n",
    "print(\"Word Embeddings:\")\n",
    "#print(word_embeddings[\"cricket\"])\n",
    "#print(word_embeddings[\"drs\"]) # please remove '#' to see the ouput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99105248",
   "metadata": {},
   "source": [
    "<h2>Contextual Embeddings</h2>\n",
    "Contextual embeddings capture the meaning of a word based on its context in a sentence or document. Unlike traditional word embeddings, contextual embeddings generate a unique representation for each occurrence of a word, taking into account its surrounding context.\n",
    "example: (BERT, GPT, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74398bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Embeddings:\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "#pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "sentence = \"I watched the thrilling cricket match between India and Australia.\"\n",
    "\n",
    "# tokenize input\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "contextual_embeddings = outputs.last_hidden_state\n",
    "\n",
    "# print contextual embeddings for some tokens\n",
    "print(\"Contextual Embeddings:\")\n",
    "#print(contextual_embeddings[0][3])  # embedding for the token 'thrilling'\n",
    "#print(contextual_embeddings[0][5])  # embedding for the token 'cricket'\n",
    "# remove '#' from above print line to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c29a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
