{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217cf880",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>Tokenization is the process of splitting a text into smaller units, such as words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e32a7f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Tokenization', 'is', 'the', 'process', 'of', 'splitting', 'a', 'text', 'into', 'smaller', 'units', ',', 'such', 'as', 'words', 'or', 'sentences', '.']\n",
      "\n",
      "Sentences: ['Tokenization is the process of splitting a text into smaller units, such as words or sentences.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "text = \"Tokenization is the process of splitting a text into smaller units, such as words or sentences.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Word Tokens:\", tokens)\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print()\n",
    "print(\"Sentences:\", sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5742d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9897b474",
   "metadata": {},
   "source": [
    "<h2>Stopword</h2> Common words that often do not convey significant meaning to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0a14807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens after Stopword Removal: ['Stopwords', 'common', 'words', 'often', 'convey', 'significant', 'meaning', 'typically', 'removed', 'text', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arjun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "text = \"Stopwords are common words that often do not convey significant meaning and are typically removed from the text.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "print(\"Filtered Tokens after Stopword Removal:\", filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee309c3b",
   "metadata": {},
   "source": [
    "<h2>Stemming and Lemmatization</h2>Stemming usually remove the suffixes to produce the root form, while lemmatization considers the context and converts words to their meaningful base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f85e9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['stem', 'and', 'lemmat', 'both', 'aim', 'to', 'reduc', 'word', 'to', 'their', 'base', 'or', 'root', 'form', ',', 'but', 'they', 'achiev', 'thi', 'in', 'slightli', 'differ', 'way', '.']\n",
      "\n",
      "Lemmatized Words: ['Stemming', 'and', 'Lemmatization', 'both', 'aim', 'to', 'reduce', 'word', 'to', 'their', 'base', 'or', 'root', 'form', ',', 'but', 'they', 'achieve', 'this', 'in', 'slightly', 'different', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "text = \"Stemming and Lemmatization both aim to reduce words to their base or root form, but they achieve this in slightly different ways.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print()\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e17af44",
   "metadata": {},
   "source": [
    "<h2>Part-of-Speech (POS) Tagging</h2> POS tagging assigns a part of speech to each word in a sentence, such as noun, verb, adjective, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "445d608d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('POS', 'NNP'), ('tagging', 'VBG'), ('assigns', 'RP'), ('a', 'DT'), ('part', 'NN'), ('of', 'IN'), ('speech', 'NN'), ('to', 'TO'), ('each', 'DT'), ('word', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('noun', 'NNS'), (',', ','), ('verb', 'NN'), (',', ','), ('adjective', 'JJ'), (',', ','), ('etc', 'FW'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"POS tagging assigns a part of speech to each word in a sentence, such as noun, verb, adjective, etc.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# perform POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a100bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
